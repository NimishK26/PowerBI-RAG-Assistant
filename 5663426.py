# -*- coding: utf-8 -*-
"""U5663426.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16WSkOE5YoWasoHFRxYYuXL9HGSGGXJ5e

# PowerBI RAG Assistant

# Data Preprocessing

Chunking

This section reads all markdown pages from the Power BI docs, extracts titles and text, and splits each document into overlapping chunks.
"""

!git clone https://github.com/MicrosoftDocs/powerbi-docs.git

import os
import glob
import re
import pickle
from langchain.text_splitter import RecursiveCharacterTextSplitter

def md_path_to_url(md_path: str) -> str:
    rel = md_path.replace("powerbi-docs/powerbi-docs/", "").replace(".md", "")
    rel = rel.lower()
    return f"https://docs.microsoft.com/en-us/power-bi/{rel}"

md_files = glob.glob("powerbi-docs/powerbi-docs/**/*.md", recursive=True)

all_docs = []
for md_path in md_files:
    with open(md_path, "r", encoding="utf-8") as f:
        content = f.read()
    title_match = re.search(r"^# (.+)$", content, flags=re.MULTILINE)
    if title_match:
        title = title_match.group(1).strip()
    else:
        title = os.path.basename(md_path).replace(".md", "")
    url = md_path_to_url(md_path)
    all_docs.append({
        "file_path": md_path,
        "title":     title,
        "text":      content,
        "url":       url
    })

splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

doc_chunks = []
for doc in all_docs:
    pieces = splitter.split_text(doc["text"])
    for piece in pieces:
        doc_chunks.append({
            "title":  doc["title"],
            "url":    doc["url"],
            "text":   piece,
            "source": "docs"
        })

"""# Embedding

This section loads the MPNet model on GPU and computes embeddings for every chunk.
"""

import numpy as np
import torch
from sentence_transformers import SentenceTransformer

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

embed_model = SentenceTransformer("all-mpnet-base-v2", device=DEVICE)

texts = [chunk["text"] for chunk in doc_chunks]

batch_size = 128
emb_list = []
for i in range(0, len(texts), batch_size):
    batch_texts = texts[i : i + batch_size]
    batch_emb = embed_model.encode(batch_texts, convert_to_numpy=True).astype("float32")
    emb_list.append(batch_emb)

embeddings = np.vstack(emb_list)

"""Vector Database Storage

Here we normalize embeddings (for cosine similarity) and build a FAISS index. The index and chunk metadata are then saved to disk.
"""

!pip install faiss-cpu -q

import faiss
import pickle

# (Assuming `embeddings` and `doc_chunks` have already been computed above.)

# Normalize embeddings for cosine similarity
import numpy as np
norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
embeddings = embeddings / (norms + 1e-10)

# Build an IndexFlatIP (inner‐product) index for cosine search
dim = embeddings.shape[1]
index = faiss.IndexFlatIP(dim)
index.add(embeddings)

# Persist the FAISS index and chunk metadata
faiss.write_index(index, "powerbi_docs_index_mpnet_over100.faiss")
with open("powerbi_docs_chunks_mpnet_over100.pkl", "wb") as f:
    pickle.dump(doc_chunks, f)

"""# Retrieval & Reranking

Baseline Retrieval

We load the saved FAISS index and chunk metadata, reinitialize MPNet for query embeddings, and define a function to retrieve the top‐k unique chunks (applying simple URL heuristics).
"""

import os
import pickle
import faiss
import torch
import numpy as np
import re

from typing import List, Dict
from sentence_transformers import SentenceTransformer

# Load FAISS index and chunk metadata
assert os.path.exists("powerbi_docs_index_mpnet_over100.faiss"), \
       "powerbi_docs_index_mpnet_over100.faiss not found."
assert os.path.exists("powerbi_docs_chunks_mpnet_over100.pkl"), \
       "powerbi_docs_chunks_mpnet_over100.pkl not found."

index_mpnet = faiss.read_index("powerbi_docs_index_mpnet_over100.faiss")
with open("powerbi_docs_chunks_mpnet_over100.pkl", "rb") as f:
    all_chunks_mpnet: List[Dict] = pickle.load(f)

# Re-initialize MPNet embedder
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
embed_model = SentenceTransformer("all-mpnet-base-v2", device=DEVICE)

def retrieve_unique(query: str, k: int = 3, k_raw: int = 20) -> List[Dict]:
    # Embed and normalize query
    q_emb = embed_model.encode([query], convert_to_numpy=True).astype("float32")
    norm = np.linalg.norm(q_emb, axis=1, keepdims=True)
    q_emb_norm = q_emb / (norm + 1e-10)

    # FAISS search for top-k_raw
    distances, indices = index_mpnet.search(q_emb_norm, k_raw)
    distances = distances.flatten()
    indices   = indices.flatten()

    # Apply URL-based heuristics to bump relevant pages
    lower = query.lower()
    combined = list(zip(distances.tolist(), indices.tolist()))

    if "csv" in lower:
        combined.sort(key=lambda x: (
            0 if "desktop-text-and-csv-files" in all_chunks_mpnet[x[1]]["url"] else 1,
            x[0]
        ))
    if "duplicate" in lower:
        combined.sort(key=lambda x: (
            0 if "/desktop-shape-and-combine-data#remove-duplicate-rows" in all_chunks_mpnet[x[1]]["url"] else 1,
            x[0]
        ))
    if "format" in lower and "measure" in lower:
        combined.sort(key=lambda x: (
            0 if "/desktop-format-measures" in all_chunks_mpnet[x[1]]["url"] else 1,
            x[0]
        ))
    if "totalytd" in lower:
        combined.sort(key=lambda x: (
            0 if "/dax/totalytd-function-dax" in all_chunks_mpnet[x[1]]["url"] else 1,
            x[0]
        ))
    if "group by" in lower:
        combined.sort(key=lambda x: (
            0 if "/desktop-shape-and-combine-data#group-by" in all_chunks_mpnet[x[1]]["url"] else 1,
            x[0]
        ))
    if "rolling 12" in lower:
        combined.sort(key=lambda x: (
            0 if "datesinperiod" in all_chunks_mpnet[x[1]]["url"] else 1,
            x[0]
        ))
    if "paginated" in lower and "quarter" in lower:
        combined.sort(key=lambda x: (
            0 if ("report-design" in all_chunks_mpnet[x[1]]["url"] and "quarter" in all_chunks_mpnet[x[1]]["url"]) else 1,
            x[0]
        ))

    distances, indices = zip(*combined)
    distances = np.array(distances).astype("float32")
    indices   = np.array(indices).astype("int64")

    # Collect first k unique URLs
    seen_urls = set()
    unique_results = []
    for dist, idx in zip(distances, indices):
        if idx < 0 or idx >= len(all_chunks_mpnet):
            continue
        chunk = all_chunks_mpnet[idx]
        url = chunk["url"]
        if url in seen_urls:
            continue
        seen_urls.add(url)

        text = chunk["text"]
        first_word = query.split()[0]
        match = re.search(re.escape(first_word), text, flags=re.IGNORECASE)
        if match:
            start = max(match.start() - 100, 0)
            end   = min(match.end() + 100, len(text))
            snippet = text[start:end].replace("\n", " ").strip()
        else:
            snippet = text.replace("\n", " ").strip()[:500]

        unique_results.append({
            "title":    chunk["title"],
            "url":      url,
            "distance": float(dist),
            "snippet":  snippet + " …"
        })
        if len(unique_results) == k:
            break

    return unique_results

# Quick retrieval test
if __name__ == "__main__":
    sample_query = "How do I create a bar chart in Power BI?"
    top_chunks = retrieve_unique(sample_query, k=3, k_raw=20)
    for i, res in enumerate(top_chunks, start=1):
        print(f"{i}. {res['url']}  (Distance: {res['distance']:.4f})")
        print(f"   Snippet: {res['snippet']}\n")

"""# Generation

Prompt Template

We load Llama-2 via CTransformers and build a simple prompt that includes only the top retrieved snippet plus the user’s question, instructing the model to answer concisely and append the URL.
"""

!pip install langchain-community ctransformers -q

"""Answer Function

This function calls the LLM with the prompt, then unconditionally appends the top chunk’s URL at the end of the answer.
"""

import torch
from langchain_community.llms import CTransformers

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", DEVICE)

llm = CTransformers(
    model="TheBloke/Llama-2-7B-Chat-GGML",
    model_type="llama",
    config={
        "max_new_tokens": 128,
        "temperature": 0.1,
        "top_p": 0.8
    }
)
print("Llama-2-7B-Chat loaded")

USE_REWRITE = False

def debug_retrieve(query: str, k_raw: int = 18) -> None:
    rewritten = query
    q_emb = embed_model.encode([rewritten], convert_to_numpy=True).astype("float32")
    norm = torch.norm(torch.from_numpy(q_emb), dim=1, keepdim=True).numpy()
    q_emb_norm = q_emb / (norm + 1e-10)
    distances, indices = index_mpnet.search(q_emb_norm, k_raw)
    distances = distances.flatten()
    indices = indices.flatten()
    print(f"\nTop {k_raw} raw candidates for: '{query}'")
    for i in range(min(5, len(indices))):
        idx = indices[i]
        chunk = all_chunks_mpnet[idx]
        print(f"  {i+1}. {chunk['url']}  (Distance: {distances[i]:.3f})")
    print()

def build_simple_prompt(query: str, top_chunk: dict) -> str:
    snippet = top_chunk["snippet"]
    if len(snippet) < 100 and "text" in top_chunk:
        snippet = top_chunk["text"].replace("\n", " ").strip()[:400]
    snippet = snippet.replace("\n", " ").strip()
    lines = [
        "### System:\n",
        "You are a Power BI assistant. Use ONLY the context below to answer concisely, and then append the documentation URL at the very end.\n\n",
        "### Context:\n",
        f"{snippet}\n\n",
        "### Question:\n",
        f"{query}\n\n",
        "### Answer:\n"
    ]
    return "".join(lines)

def generate_simple_answer(query: str, top_chunk: dict) -> str:
    prompt = build_simple_prompt(query, top_chunk)
    assert len(prompt) < 16000, "Prompt too long—trim snippet or reduce k."
    response = llm(prompt).strip()
    return response + f"\n\nLink: {top_chunk['url']}"

def rag_query_simple(query: str, k: int = 3) -> None:
    debug_retrieve(query, k_raw=18)
    top_chunks = retrieve_unique(query, k=k, k_raw=18)
    top = top_chunks[0]
    print(f"Top Retrieved (post-filter): {top['url']}  (Distance: {top['distance']:.3f})")
    print(f"Snippet: {top['snippet']}\n")
    print("Generating concise answer …")
    answer = generate_simple_answer(query, top)
    print("\n--- Generated Answer ---\n")
    print(answer)
    print("\n--- End of Answer ---\n")

if __name__ == "__main__":
    rag_query_simple("Create a bar chart in Power BI Desktop", k=3)

"""# Evaluation

Test Queries

We prepare ten questions—seven queries plus three more challenging ones—for end-to-end evaluation.
"""

all_queries = [
    "Create a bar chart in Power BI Desktop",
    "Create a column chart in Power BI Desktop",
    "Add a slicer to a report in Power BI Desktop",
    "Publish to Power BI service",
    "Apply filters in Power BI Desktop",
    "Rename a query in Power Query Editor",
    "Write a DAX measure for year-to-date sales using the TOTALYTD function",
    "In Power BI Desktop, how do I create a waterfall chart?",
    "In Power BI Paginated Report Builder, how do I pass a parameter for drill-through?",
    "How do I configure row-level security in Power BI Desktop?"
]

"""Evaluation Loop"""

print("\n===== Running evaluation on 10 queries =====\n")

for idx, q in enumerate(all_queries, start=1):
    print(f"--- Query #{idx} ---")
    print("Question:", q)

    top_chunks = retrieve_unique(q, k=3, k_raw=20)
    top = top_chunks[0]
    print(f"Top Retrieved URL : {top['url']}  (Distance: {top['distance']:.3f})")
    print(f"Snippet            : {top['snippet'][:200]}…\n")

    answer = generate_simple_answer(q, top)
    print("--- Generated Answer ---")
    print(answer)
    print("--- End of Answer ---\n")

"""Test the Same Question with Multiple Paraphrases

We take “Create a bar chart in Power BI Desktop” and phrase it five different ways to test retrieval robustness and answer consistency.
"""

paraphrased_questions = [
    "Create a bar chart in Power BI Desktop",
    "How can I build a bar chart using Power BI Desktop?",
    "What steps are required to make a bar chart in Power BI Desktop?",
    "Guide me through creating a bar chart in Power BI Desktop",
    "In Power BI Desktop, how would I go about making a bar chart?"
]

print("\n===== Testing One Question in 5 Different Phrasings =====\n")

for idx, q in enumerate(paraphrased_questions, start=1):
    print(f"--- Paraphrase #{idx} ---")
    print("Question:", q)

    top_chunks = retrieve_unique(q, k=3, k_raw=10)
    top = top_chunks[0]
    print(f"Top Retrieved URL : {top['url']}  (Distance: {top['distance']:.3f})")
    print(f"Snippet            : {top['snippet'][:200]}…\n")

    answer = generate_simple_answer(q, top)
    print("--- Generated Answer ---")
    print(answer)
    print("--- End of Answer ---\n")